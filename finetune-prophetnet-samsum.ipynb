{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install sentencepiece\n!pip install --upgrade accelerate\n!pip install rouge.score nltk py7zr\n!pip install datasets\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T04:55:22.760090Z","iopub.execute_input":"2023-08-16T04:55:22.760956Z","iopub.status.idle":"2023-08-16T04:56:30.278804Z","shell.execute_reply.started":"2023-08-16T04:55:22.760920Z","shell.execute_reply":"2023-08-16T04:56:30.277622Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.20.3)\nCollecting accelerate\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.20.3\n    Uninstalling accelerate-0.20.3:\n      Successfully uninstalled accelerate-0.20.3\nSuccessfully installed accelerate-0.21.0\nCollecting rouge.score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting py7zr\n  Downloading py7zr-0.20.6-py3-none-any.whl (66 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge.score) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge.score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge.score) (1.16.0)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.6.7)\nCollecting pycryptodomex>=3.6.6 (from py7zr)\n  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr)\n  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr)\n  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nCollecting brotli>=1.0.9 (from py7zr)\n  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr)\n  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nBuilding wheels for collected packages: rouge.score\n  Building wheel for rouge.score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge.score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=c452346e9e136d1f213e6dfe7b5fa917b29446c6123b328bf64affd0356875e2\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge.score\nInstalling collected packages: brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, rouge.score, py7zr\nSuccessfully installed brotli-1.0.9 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.6 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 rouge.score-0.1.2\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\nfrom datasets import load_dataset, load_metric, load_from_disk ,DatasetDict\nimport numpy as np\nimport nltk\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T04:56:30.281179Z","iopub.execute_input":"2023-08-16T04:56:30.281630Z","iopub.status.idle":"2023-08-16T04:56:33.876145Z","shell.execute_reply.started":"2023-08-16T04:56:30.281585Z","shell.execute_reply":"2023-08-16T04:56:33.875188Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"data = load_dataset('samsum',split=[\"train[:5000]\",\"validation[:625]\",\"test[:625]\"])\ndata = DatasetDict({'train':data[0],'validation':data[1],'test':data[2]})\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T04:56:33.877780Z","iopub.execute_input":"2023-08-16T04:56:33.878771Z","iopub.status.idle":"2023-08-16T04:56:39.950736Z","shell.execute_reply.started":"2023-08-16T04:56:33.878735Z","shell.execute_reply":"2023-08-16T04:56:39.949853Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b55e8d03dcc47b1aa194e8bab14177c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/770 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"730855cbe50d4ecd87c0adbf8cc1a61e"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset samsum/samsum (download: 2.81 MiB, generated: 10.04 MiB, post-processed: Unknown size, total: 12.85 MiB) to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5025c6502a2a4558b25d52bf695dd5e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset samsum downloaded and prepared to /root/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec9a6a2e911345da80fb787f6d8d0edb"}},"metadata":{}}]},{"cell_type":"code","source":"metric = load_metric('rouge')","metadata":{"execution":{"iopub.status.busy":"2023-08-16T04:56:39.953008Z","iopub.execute_input":"2023-08-16T04:56:39.953467Z","iopub.status.idle":"2023-08-16T04:56:40.555040Z","shell.execute_reply.started":"2023-08-16T04:56:39.953431Z","shell.execute_reply":"2023-08-16T04:56:40.554156Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15d4be4c28374a91a3a4c3ed0da81816"}},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import ProphetNetForConditionalGeneration, ProphetNetTokenizer\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T04:56:40.556387Z","iopub.execute_input":"2023-08-16T04:56:40.556831Z","iopub.status.idle":"2023-08-16T04:56:51.397020Z","shell.execute_reply.started":"2023-08-16T04:56:40.556794Z","shell.execute_reply":"2023-08-16T04:56:51.396076Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the tokenizer and model\ntokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased',use_fast=False)\nmodel = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T04:56:56.555250Z","iopub.execute_input":"2023-08-16T04:56:56.555712Z","iopub.status.idle":"2023-08-16T04:57:13.698555Z","shell.execute_reply.started":"2023-08-16T04:56:56.555673Z","shell.execute_reply":"2023-08-16T04:57:13.697539Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)prophetnet.tokenizer:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6da6d67395d47a0860c786e5661ebbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c00fdb35744abdb9f1289858067923"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d1206dcf5a4d3b96661e48a33e2793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad524402c2da48d9bd8668e8a3b6c276"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/1.57G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f0f3e3d7b9b4ec19d444ae166a092d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"546a358430884fe28d4deaaf8b4dad67"}},"metadata":{}}]},{"cell_type":"code","source":"max_input = 512\nmax_target = 128\n#tokenizer = transformers.PegasusTokenizer.from_pretrained(model_checkpoints)\n\n\ndef preprocess_data(data_to_process):\n  #get the dialogue text\n  inputs = [dialogue for dialogue in data_to_process['dialogue']]\n  #tokenize text\n  model_inputs = tokenizer(inputs,  max_length=max_input, padding='max_length', truncation=True)\n\n  #tokenize labels\n  with tokenizer.as_target_tokenizer():\n    targets = tokenizer(data_to_process['summary'], max_length=max_target, padding='max_length', truncation=True)\n\n  model_inputs['labels'] = targets['input_ids']\n  #reuturns input_ids, attention_masks, labels\n  return model_inputs\n\n\ntokenize_data = data.map(preprocess_data, batched = True, remove_columns=['id', 'dialogue', 'summary'])\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T04:57:21.082781Z","iopub.execute_input":"2023-08-16T04:57:21.083172Z","iopub.status.idle":"2023-08-16T04:57:45.768476Z","shell.execute_reply.started":"2023-08-16T04:57:21.083132Z","shell.execute_reply":"2023-08-16T04:57:45.767540Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02a32d4205cd4e35b086094a95ac0b14"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fa621df2bdb48e28990f66c50ccebd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7127bfa129f4387bf4605e445a945a7"}},"metadata":{}}]},{"cell_type":"code","source":"#import transformers\n#collator to create batches. It preprocess data with the given tokenizer\ncollator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)\n\n\n#####################\n# metrics\n# compute rouge for evaluation\n#####################\n\ndef compute_rouge(pred):\n  predictions, labels = pred\n  #decode the predictions\n  decode_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n  #decode labels\n  decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n  #compute results\n  res = metric.compute(predictions=decode_predictions, references=decode_labels, use_stemmer=True)\n  #get %\n  res = {key: value.mid.fmeasure * 100 for key, value in res.items()}\n\n  pred_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n  res['gen_len'] = np.mean(pred_lens)\n\n  return {k: round(v, 4) for k, v in res.items()}\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T04:57:50.199866Z","iopub.execute_input":"2023-08-16T04:57:50.200252Z","iopub.status.idle":"2023-08-16T04:57:50.227801Z","shell.execute_reply.started":"2023-08-16T04:57:50.200220Z","shell.execute_reply":"2023-08-16T04:57:50.226930Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"args = transformers.Seq2SeqTrainingArguments(\n    'samsum-summ',\n    evaluation_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=6,\n    per_device_eval_batch_size= 6,\n    gradient_accumulation_steps=3,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=10,\n    predict_with_generate=True,\n    eval_accumulation_steps=1,\n    fp16=True\n    )\n#only CUDA available -> fp16=True\ntrainer = transformers.Seq2SeqTrainer(\n    model,\n    args,\n    train_dataset=tokenize_data['train'],\n    eval_dataset=tokenize_data['validation'],\n    data_collator=collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_rouge\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T04:58:02.503655Z","iopub.execute_input":"2023-08-16T04:58:02.504023Z","iopub.status.idle":"2023-08-16T04:58:08.262715Z","shell.execute_reply.started":"2023-08-16T04:58:02.503991Z","shell.execute_reply":"2023-08-16T04:58:08.261684Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-16T04:58:08.264680Z","iopub.execute_input":"2023-08-16T04:58:08.265036Z","iopub.status.idle":"2023-08-16T09:45:59.552771Z","shell.execute_reply.started":"2023-08-16T04:58:08.265002Z","shell.execute_reply":"2023-08-16T09:45:59.551711Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230816_045820-cyoil2ud</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/soumikdey810/huggingface/runs/cyoil2ud' target=\"_blank\">fluent-gorge-45</a></strong> to <a href='https://wandb.ai/soumikdey810/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/soumikdey810/huggingface' target=\"_blank\">https://wandb.ai/soumikdey810/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/soumikdey810/huggingface/runs/cyoil2ud' target=\"_blank\">https://wandb.ai/soumikdey810/huggingface/runs/cyoil2ud</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2780' max='2780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2780/2780 4:47:01, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.439369</td>\n      <td>49.601200</td>\n      <td>24.155700</td>\n      <td>40.049000</td>\n      <td>40.027800</td>\n      <td>24.796800</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.459400</td>\n      <td>0.402403</td>\n      <td>52.384700</td>\n      <td>26.501500</td>\n      <td>43.179000</td>\n      <td>43.062200</td>\n      <td>25.283200</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.459400</td>\n      <td>0.407527</td>\n      <td>52.055000</td>\n      <td>26.481400</td>\n      <td>42.234700</td>\n      <td>42.226800</td>\n      <td>28.524800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.353900</td>\n      <td>0.396456</td>\n      <td>51.659800</td>\n      <td>26.096100</td>\n      <td>41.214400</td>\n      <td>41.152000</td>\n      <td>32.672000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.353900</td>\n      <td>0.403361</td>\n      <td>50.153100</td>\n      <td>24.894700</td>\n      <td>39.565900</td>\n      <td>39.467900</td>\n      <td>35.283200</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.270500</td>\n      <td>0.416883</td>\n      <td>49.322800</td>\n      <td>24.654400</td>\n      <td>38.754600</td>\n      <td>38.671100</td>\n      <td>38.323200</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.270500</td>\n      <td>0.427659</td>\n      <td>48.733500</td>\n      <td>24.128300</td>\n      <td>38.240700</td>\n      <td>38.174500</td>\n      <td>39.096000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.218600</td>\n      <td>0.439133</td>\n      <td>49.240500</td>\n      <td>24.503400</td>\n      <td>38.561900</td>\n      <td>38.509600</td>\n      <td>39.184000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.186100</td>\n      <td>0.444055</td>\n      <td>48.622000</td>\n      <td>24.146500</td>\n      <td>38.017700</td>\n      <td>37.962900</td>\n      <td>39.880000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.186100</td>\n      <td>0.449128</td>\n      <td>48.761100</td>\n      <td>23.958200</td>\n      <td>37.979600</td>\n      <td>37.934500</td>\n      <td>39.387200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2780, training_loss=0.4647073334069561, metrics={'train_runtime': 17270.9255, 'train_samples_per_second': 2.895, 'train_steps_per_second': 0.161, 'total_flos': 5.51449264128e+16, 'train_loss': 0.4647073334069561, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"predict_results = trainer.predict(\n            tokenize_data['test'], metric_key_prefix=\"predict\", max_length=128, num_beams=5\n        )\n","metadata":{"execution":{"iopub.status.busy":"2023-08-16T09:51:16.624542Z","iopub.execute_input":"2023-08-16T09:51:16.624940Z","iopub.status.idle":"2023-08-16T09:58:47.163867Z","shell.execute_reply.started":"2023-08-16T09:51:16.624907Z","shell.execute_reply":"2023-08-16T09:58:47.162039Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"predictions = tokenizer.batch_decode(\n                    predict_results.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n                )","metadata":{"execution":{"iopub.status.busy":"2023-08-16T10:13:35.798234Z","iopub.execute_input":"2023-08-16T10:13:35.798642Z","iopub.status.idle":"2023-08-16T10:13:37.289347Z","shell.execute_reply.started":"2023-08-16T10:13:35.798610Z","shell.execute_reply":"2023-08-16T10:13:37.288123Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nlabels = np.where(predict_results.label_ids != -100, predict_results.label_ids, tokenizer.pad_token_id)\nactual = tokenizer.batch_decode(\n                    labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n                )\nfrom nltk.tokenize import sent_tokenize\npreds = [pred.strip() for pred in predictions]\nlabels = [label.strip() for label in actual]\n\n# rougeLSum expects newline after each sentence\npreds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds] #predicted summary\nlabels = [\"\\n\".join(sent_tokenize(label)) for label in labels] #actual summary","metadata":{"execution":{"iopub.status.busy":"2023-08-16T10:13:37.291195Z","iopub.execute_input":"2023-08-16T10:13:37.291816Z","iopub.status.idle":"2023-08-16T10:13:38.806727Z","shell.execute_reply.started":"2023-08-16T10:13:37.291775Z","shell.execute_reply":"2023-08-16T10:13:38.805633Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame({\"Dialogue\":data[\"test\"][\"dialogue\"],\"Actual Summary\":data[\"test\"][\"summary\"],'Model Summary':preds})\ndf.to_csv('outputProfetnetSamsum.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T10:14:12.246207Z","iopub.execute_input":"2023-08-16T10:14:12.246903Z","iopub.status.idle":"2023-08-16T10:14:12.285315Z","shell.execute_reply.started":"2023-08-16T10:14:12.246865Z","shell.execute_reply":"2023-08-16T10:14:12.284190Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}