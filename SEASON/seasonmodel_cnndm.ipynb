{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kZSkBZY1J9N"
   },
   "outputs": [],
   "source": [
    "!pip install rouge-score py7zr\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install tokenizers\n",
    "!pip install evaluate\n",
    "!pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G799Zsv1WRo"
   },
   "outputs": [],
   "source": [
    "!python -c \"import nltk; nltk.download('punkt');\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1m6r0wmKegm"
   },
   "outputs": [],
   "source": [
    "mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZRKM35yKqvK"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "from functools import partial\n",
    "\n",
    "import datasets\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from nltk import sent_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = datasets.load_dataset('cnn_dailymail', name='3.0.0',split=[\"train[:10000]\",\"validation[:1000]\",\"test[:1000]\"])\n",
    "dataset = datasets.DatasetDict({'train':dataset[0],'validation':dataset[1],'test':dataset[2]})\n",
    "src_text_column_name, tgt_text_column_name = \"article\", \"highlights\"\n",
    "max_source_length, max_target_length = 1024, 128\n",
    "n_proc = 40\n",
    "\n",
    "# Since bos_token is used as the beginning of target sequence,\n",
    "# we use mask_token to represent the beginning of each sentence.\n",
    "bosent_token = \"<mask>\"\n",
    "bosent_token_id = 50264\n",
    "\n",
    "rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "\n",
    "def convert_to_features(\n",
    "        examples: Any,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        padding: str,\n",
    "        max_source_length: int,\n",
    "        max_target_length: int,\n",
    "        src_text_column_name: str,\n",
    "        tgt_text_column_name: str,\n",
    "):\n",
    "    inputs, targets = [], []\n",
    "    all_sent_rouge_scores = []\n",
    "    for i in range(len(examples[src_text_column_name])):\n",
    "        if examples[src_text_column_name][i] is not None and examples[tgt_text_column_name][i] is not None:\n",
    "            input_sentences = sent_tokenize(examples[src_text_column_name][i])\n",
    "            target_sentences = examples[tgt_text_column_name][i].strip()\n",
    "            rouge_scores = []\n",
    "            for sent in input_sentences:\n",
    "                rouge_scores.append(rouge_scorer.score(target_sentences, sent)['rougeL'].fmeasure)\n",
    "            # todo: add bos_token this way is unsafe\n",
    "            inputs.append(bosent_token.join(input_sentences))\n",
    "            targets.append(target_sentences.replace('\\n', ' ').replace('  ', ' '))\n",
    "            all_sent_rouge_scores.append(rouge_scores)\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # replace bos_token_id at the begining of document with bosent_token_id\n",
    "    for i in range(len(model_inputs['input_ids'])):\n",
    "        model_inputs['input_ids'][i][0] = bosent_token_id\n",
    "\n",
    "    all_token_sent_id = []\n",
    "    for sent_tokens in model_inputs['input_ids']:\n",
    "        sid = -1\n",
    "        token_sent_id = []\n",
    "        for tid in sent_tokens:\n",
    "            if tid == bosent_token_id:\n",
    "                sid += 1\n",
    "            if tid == tokenizer.eos_token_id or tid == tokenizer.pad_token_id:\n",
    "                sid = -1\n",
    "            token_sent_id.append(sid)\n",
    "        all_token_sent_id.append(token_sent_id)\n",
    "\n",
    "    all_token_info_dist = []\n",
    "    all_sent_bos_idx = []\n",
    "    for token_sent_id, sent_rouge_scores in zip(all_token_sent_id, all_sent_rouge_scores):\n",
    "        sent_rouge_scores = sent_rouge_scores[:max(token_sent_id) + 1]  # truncation\n",
    "        sent_bos_idx = []\n",
    "        token_info_dist = []\n",
    "        bos_idx = 0\n",
    "        for sid in range(max(token_sent_id) + 1):\n",
    "            tnum = token_sent_id.count(sid)\n",
    "            sent_score = sent_rouge_scores[sid]\n",
    "            token_info_dist.extend([sent_score for _ in range(tnum)])\n",
    "            sent_bos_idx.extend([bos_idx for _ in range(tnum)])\n",
    "            bos_idx += tnum\n",
    "        token_info_dist.extend([-1 for _ in range(token_sent_id.count(-1))])\n",
    "        all_token_info_dist.append(token_info_dist)\n",
    "        sent_bos_idx.extend([0 for _ in range(token_sent_id.count(-1))])\n",
    "        all_sent_bos_idx.append(sent_bos_idx)\n",
    "\n",
    "    for i in range(len(all_token_sent_id)):\n",
    "        for j in range(len(all_token_sent_id[i])):\n",
    "            all_token_sent_id[i][j] += 1\n",
    "\n",
    "    model_inputs['info_distribution'] = all_token_info_dist\n",
    "    model_inputs['sentence_bos_index'] = all_sent_bos_idx\n",
    "    model_inputs['sent_id'] = all_token_sent_id\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\", use_fast=False)\n",
    "\n",
    "convert_to_features = partial(\n",
    "    convert_to_features,\n",
    "    tokenizer=tokenizer,\n",
    "    padding='max_length',\n",
    "    max_source_length=max_source_length,\n",
    "    max_target_length=max_target_length,\n",
    "    src_text_column_name=src_text_column_name,\n",
    "    tgt_text_column_name=tgt_text_column_name,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    convert_to_features,\n",
    "    batched=True,\n",
    "    num_proc=n_proc,\n",
    ")\n",
    "\n",
    "cols_to_keep = [\"input_ids\", \"attention_mask\", \"labels\", \"info_distribution\", \"sentence_bos_index\", \"sent_id\"]\n",
    "dataset.set_format(columns=cols_to_keep)\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    with open(f'data/{split}.json', 'w') as outfile:\n",
    "        for i, example in enumerate(dataset[split]):\n",
    "            json_string = json.dumps(example)\n",
    "            outfile.write(json_string + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oA1i3UPcL8Fi"
   },
   "outputs": [],
   "source": [
    "!python run.py \\\n",
    "    --model_name_or_path facebook/bart-large-cnn \\\n",
    "    --do_train \\\n",
    "    --train_file data/train.json \\\n",
    "    --validation_file data/validation.json \\\n",
    "    --test_file data/test.json \\\n",
    "    --output_dir outputs/train \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --per_device_eval_batch_size 3 \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --warmup_steps 1500 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --max_grad_norm 0.1 \\\n",
    "    --metric_for_best_model rougeL \\\n",
    "    --evaluation_strategy epoch \\\n",
    "    --save_strategy epoch \\\n",
    "    --save_total_limit 2\\\n",
    "    --load_best_model_at_end True\\\n",
    "    --fp16 true \\\n",
    "    --bosent_token_id 50264 \\\n",
    "    --encoder_loss_ratio 1.0 \\\n",
    "    --encoder_label_smoothing 0.1 \\\n",
    "    --encoder_label_smoothing_type adjacent \\\n",
    "    --lower_saliency_threshold 0.125 \\\n",
    "    --higher_saliency_threshold 0.230 \\\n",
    "    --marginal_distribution true \\\n",
    "    --marginal_temperature 0.5 \\\n",
    "    --num_beams 5 \\\n",
    "    --max_length 128 \\\n",
    "    --min_length 20 \\\n",
    "    --length_penalty 1.5 \\\n",
    "    --no_repeat_ngram_size 3 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apTWrdA71Q8F",
    "outputId": "c2734c12-27d1-4609-a5ac-28352ccf8b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-25 07:01:32.880456: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "07/25/2023 07:01:35 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2066: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "[WARNING|logging.py:280] 2023-07-25 07:01:46,245 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100% 334/334 [1:55:36<00:00, 20.77s/it]\n",
      "***** predict metrics *****\n",
      "  predict_gen_len            =     46.602\n",
      "  predict_loss               =     1.7076\n",
      "  predict_rouge1             =    38.0718\n",
      "  predict_rouge2             =    16.7868\n",
      "  predict_rougeL             =    27.8878\n",
      "  predict_rougeLsum          =      34.67\n",
      "  predict_runtime            = 1:55:56.51\n",
      "  predict_samples            =       1000\n",
      "  predict_samples_per_second =      0.144\n",
      "  predict_steps_per_second   =      0.048\n"
     ]
    }
   ],
   "source": [
    "!python run.py \\\n",
    "    --model_name_or_path /kaggle/working/outputs/season_cnndm \\\n",
    "    --tokenizer_name facebook/bart-large-cnn \\\n",
    "    --do_predict \\\n",
    "    --train_file data/train.json \\\n",
    "    --validation_file data/validation.json \\\n",
    "    --test_file data/test.json \\\n",
    "    --output_dir outputs/inference \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 3 \\\n",
    "    --num_beams 5 \\\n",
    "    --max_length 128 \\\n",
    "    --min_length 20 \\\n",
    "    --length_penalty 1.5 \\\n",
    "    --no_repeat_ngram_size 3 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --predict_with_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRrKNEC41wA6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
